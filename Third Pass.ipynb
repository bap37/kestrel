{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedc3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dustbi_simulator import *\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084facff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import Planck18\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677cac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2p/hm6bd5n17d5g5kpsm6s7vf3r0002g2/T/ipykernel_67812/1071556963.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dfdata['MU'] = Planck18.distmod(dfdata.zHD.values).value\n",
      "/var/folders/2p/hm6bd5n17d5g5kpsm6s7vf3r0002g2/T/ipykernel_67812/1071556963.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dfdata['SIM_EBV'] = dfdata.SIM_AV/dfdata.SIM_RV\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"INPUT_DES5YR_D2D.FITRES\", comment=\"#\", sep='\\s+')\n",
    "\n",
    "df['SIM_EBV'] = df.SIM_AV/df.SIM_RV\n",
    "\n",
    "df['MU'] = Planck18.distmod(df.zHD.values).value\n",
    "\n",
    "\n",
    "\n",
    "dfdata = pd.read_csv(\"SIMS_FOR_TESTING/FITOPT000.FITRES.gz\", \n",
    "                     comment=\"#\", sep=r'\\s+')\n",
    "\n",
    "dfdata['MU'] = Planck18.distmod(dfdata.zHD.values).value\n",
    "\n",
    "#dfdata = pd.read_csv(\"../INVERSE_H0/D5YR_DATA/FITOPT000_MUOPT000.FITRES.gz\", comment=\"#\", sep=r'\\s+')\n",
    "\n",
    "try:\n",
    "    dfdata['SIM_EBV'] = dfdata.SIM_AV/dfdata.SIM_RV\n",
    "except:\n",
    "    print(\"eh.\")\n",
    "\n",
    "dfdata = dfdata.loc[dfdata.IDSURVEY == 10]\n",
    "dfdata = dfdata.loc[dfdata.PROB_SNNV19 >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221231f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_dict = {\n",
    "    \"SIM_c\"   : (-0.5, 0.5),\n",
    "    \"SIM_RV\"  : (1.5, 5),\n",
    "    \"SIM_EBV\" : (0,1),\n",
    "    \"SIM_beta\": (0.5,4),\n",
    "}\n",
    "\n",
    "function_dict = {\n",
    "    \"SIM_c\"   : DistGaussian,\n",
    "    \"SIM_RV\"  : DistGaussian,\n",
    "    \"SIM_EBV\" : DistExponential,\n",
    "    \"SIM_beta\": DistGaussian,\n",
    "}\n",
    "\n",
    "split_dict = {\n",
    "#    \"SIM_RV\":[\"HOST_LOGMASS\", 10],\n",
    "    \"SIM_EBV\":['HOST_LOGMASS', 10],\n",
    "#    'SIM_c':['HOST_LOGMASS', 10]\n",
    "}\n",
    "\n",
    "\n",
    "#Prior dict is a weird one; it should be a tuple for each parameter and then a boolean statement.\n",
    "\n",
    "split_dict = {}\n",
    "\n",
    "\n",
    "priors_dict = {\n",
    "    \n",
    "    \"SIM_c\"   : [(-0.2, 0), (0.0, 0.1), ],\n",
    "    \"SIM_RV\"  : [(1.5,4), (0,2), ],\n",
    "    \"SIM_EBV\" : [(0.05, 0.3)],\n",
    "    \"SIM_beta\": [(0,3), (0,1), ],\n",
    "    \n",
    "}\n",
    "\n",
    "latex_dict = {\n",
    "    \n",
    "    'DistExponential':[r'$\\tau$'],\n",
    "    'DistGaussian':[r'$\\mu$', r'$\\sigma$'],\n",
    "    'SIM_c':r\"$c_{\\rm int}$\",\n",
    "    'SIM_beta':r\"$\\beta_{\\rm int}$\",\n",
    "    'SIM_RV':r\"$R_V$\",\n",
    "    'SIM_EBV':r\"$EBV$\",\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "dicts = [bounds_dict, function_dict, split_dict, priors_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca1051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be81940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total priors added: 7\n",
      "[0] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[1] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[2] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[3] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[4] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[5] <class 'sbi.utils.torchutils.BoxUniform'>\n",
      "[6] <class 'sbi.utils.torchutils.BoxUniform'>\n"
     ]
    }
   ],
   "source": [
    "param_names = ['SIM_c', 'SIM_RV', 'SIM_beta', 'SIM_EBV']\n",
    "#param_names = ['SIM_c']\n",
    "\n",
    "\n",
    "params_to_fit = parameter_generation(param_names, dicts)\n",
    "priors = prior_generator(param_names, dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ee03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = build_layout(params_to_fit, dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334dbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d707a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_condition_on = ['c', 'mB', 'x1', 'zHD', 'cERR', 'mBERR', 'x1ERR', 'MU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c32dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulatinator = make_simulator(layout, df, param_names, parameters_to_condition_on, dicts, dfdata, is_split=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077b6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58111230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "ndim = len(parameters_to_condition_on)#+1\n",
    "\n",
    "if any(p in split_dict for p in param_names): #check early to see if we need to split anything. \n",
    "    ndim *= 2\n",
    "    \n",
    "print(ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9266b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54700df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_simulator(theta_batch):\n",
    "    return torch.stack([simulatinator(theta) for theta in theta_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_simulator(theta_batch):\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(simulatinator)(theta)\n",
    "        for theta in theta_batch\n",
    "    )\n",
    "    return torch.stack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb92e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi import analysis as analysis\n",
    "\n",
    "# sbi\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ec48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prior, simulator, consistency\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(priors)\n",
    "simulation_wrapper = process_simulator(simulatinator, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulation_wrapper, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63690751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "502d0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import SNPE\n",
    "from sbi.utils import MultipleIndependent\n",
    "\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178d092",
   "metadata": {},
   "source": [
    "# Potentially Upgraded Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23e2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi import analysis as analysis\n",
    "from sbi.inference import SNPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "class PopulationEmbeddingFull(nn.Module):\n",
    "    def __init__(self, input_dim=ndim, hidden_dim=64, output_dim=32):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.phi(x)                              # (batch, N, hidden)\n",
    "        w = torch.softmax(self.attention(h), dim=1)   # May need to train this to only run on errors? \n",
    "                                                      # Right now runs on everything ... \n",
    "        h = (h * w).sum(dim=1)                        # (batch, hidden)\n",
    "        return self.rho(h)\n",
    "    \n",
    "#might need to standardise errors and signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d834cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import SNPE\n",
    "from sbi.utils import MultipleIndependent\n",
    "\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "density_estimator = posterior_nn(\n",
    "    model=\"nsf\", #switch to nsf if interested \n",
    "    embedding_net=PopulationEmbeddingFull(input_dim=ndim)\n",
    ")\n",
    "\n",
    "inference = SNPE(\n",
    "    prior=priors,\n",
    "    density_estimator=density_estimator, \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf09d29",
   "metadata": {},
   "source": [
    "# Permutation Invariant Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding, PermutationInvariantEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684483ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_trial_net = FCEmbedding(\n",
    "    input_dim=ndim,\n",
    "    num_hiddens=40,\n",
    "    num_layers=2,\n",
    "    output_dim=ndim,\n",
    ")\n",
    "embedding_net = PermutationInvariantEmbedding(\n",
    "    single_trial_net,\n",
    "    trial_net_output_dim=ndim,\n",
    "    num_layers=1,\n",
    "    num_hiddens=10,\n",
    "    output_dim=ndim,\n",
    ")\n",
    "density_estimator = posterior_nn(\"nsf\", embedding_net=embedding_net)\n",
    "\n",
    "\n",
    "\n",
    "inference = SNPE(\n",
    "    prior=priors,\n",
    "    density_estimator=density_estimator, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2c393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de3a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df484a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "batch_size = 50\n",
    "num_simulations = 4000\n",
    "save_path = \"simulations_v2.pt\"\n",
    "\n",
    "# If the file already exists, start fresh\n",
    "if os.path.exists(save_path):\n",
    "    os.remove(save_path)\n",
    "\n",
    "for start in range(0, num_simulations, batch_size):\n",
    "    current_bs = min(batch_size, num_simulations - start)\n",
    "\n",
    "    # Sample and simulate\n",
    "    theta_batch = priors.sample((current_bs,))\n",
    "    x_batch = batched_simulator(theta_batch)\n",
    "\n",
    "    # Append to SBI inference\n",
    "    inference.append_simulations(theta_batch, x_batch)\n",
    "\n",
    "    # Save incrementally\n",
    "    if start == 0:\n",
    "        # First batch, create the file\n",
    "        torch.save({'theta': theta_batch, 'x': x_batch}, save_path)\n",
    "    else:\n",
    "        # Load existing data\n",
    "        data = torch.load(save_path)\n",
    "        data['theta'] = torch.cat([data['theta'], theta_batch], dim=0)\n",
    "        data['x'] = torch.cat([data['x'], x_batch], dim=0)\n",
    "        torch.save(data, save_path)\n",
    "\n",
    "    print(f\"Appended {start + current_bs}/{num_simulations} simulations and saved incrementally.\")\n",
    "\n",
    "print(f\"All simulations saved incrementally to '{save_path}'\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "elapsed = end - start_time\n",
    "#print(f'Time taken: {elapsed:.6f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293c5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Time taken: {elapsed/60:.6f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811c12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found 1846 NaN simulations and 0 Inf simulations. They will be excluded from training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 19"
     ]
    }
   ],
   "source": [
    "inference.append_simulations(theta_batch, x_batch)\n",
    "\n",
    "density_estimator = inference.train(validation_fraction=0.1,\n",
    "                                   show_train_summary=True)\n",
    "#force_first_round_loss \n",
    "#when true, only compute standard NPE loss;\n",
    "#think about moving to the S of SNPE eventually... \n",
    "\n",
    "print(\"\\n inferred successfully\")\n",
    "\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "torch.save(posterior, \"posterior.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef048cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83953b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd5df40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2p/hm6bd5n17d5g5kpsm6s7vf3r0002g2/T/ipykernel_67812/3416222729.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"simulations_v2.pt\")\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(\"simulations_v2.pt\")\n",
    "theta_batch = data[\"theta\"]\n",
    "x_batch = data[\"x\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06b310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e5e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfcb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d3562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(param_names, parameters_to_condition_on, split_dict, dfdata, ):\n",
    "    \n",
    "    #salt_mcmc = start_distance()\n",
    "       \n",
    "    output_distribution = preprocess_input_distribution(\n",
    "        dfdata, parameters_to_condition_on+['x0', 'x0ERR', 'MU']\n",
    "    )\n",
    "    \n",
    "    #salt_mcmc.run(\n",
    "    #    output_distribution['x0'],\n",
    "    #    output_distribution['x0ERR'],\n",
    "    #    output_distribution['x1'],\n",
    "    #    output_distribution['x1ERR'],\n",
    "    #    output_distribution['c'],\n",
    "    #    output_distribution['cERR'],\n",
    "    #    output_distribution['MU']\n",
    "    #    )\n",
    "\n",
    "    #MURES = add_distance(salt_mcmc, output_distribution)\n",
    "    #output_distribution['MURES'] = MURES\n",
    "    \n",
    "    is_split = False\n",
    "    if any(p in split_dict for p in param_names):\n",
    "        is_split = True#check early to see if we need to split anything. \n",
    "    \n",
    "    if is_split:\n",
    "    \n",
    "        matching = [p for p in param_names if p in split_dict]\n",
    "        name = matching[0]\n",
    "\n",
    "        split_param = split_dict[name][0]\n",
    "        split_val   = split_dict[name][1]\n",
    "\n",
    "        split_tensor = torch.tensor(\n",
    "            dfdata[split_param].to_numpy(),\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "        x = split_outputs(\n",
    "            output_distribution,\n",
    "            split_tensor,\n",
    "            split_val,\n",
    "            parameters_to_condition_on#+['MURES']\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        x = torch.stack(\n",
    "            [output_distribution[p] for p in parameters_to_condition_on],#+['MURES']],\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess_data(param_names, parameters_to_condition_on, split_dict, dfdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf79cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = priors.sample()\n",
    "\n",
    "new_x = simulatinator(true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = unspool_labels(param_names, dicts, latex_dict, function_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6f977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posterior_samples = posterior.sample((50000,), x=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = analysis.pairplot(\n",
    "    posterior_samples,\n",
    "    labels=labels\n",
    "\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7711ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0f5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = posterior_samples.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3987daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_params = true_vals#torch.tensor([-0.07, 0.053, 2, 0.95, 2.07, 0.22, 0.15, 0.12,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27837a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(theta_hat)):\n",
    "    \n",
    "\n",
    "    delta = theta_hat[n] - true_params[n]\n",
    "    sigma = delta/posterior_samples.std(0)[n]\n",
    "    \n",
    "    string = rf\"{labels[n]} = {theta_hat[n]:.3f} +/- {posterior_samples.std(0)[n]:.3f} which is {sigma:.3f} $\\sigma$\"\n",
    "    \n",
    "    display(Math(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0385aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb98263",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulatinator = make_simulator(layout, df, param_names, parameters_to_condition_on, dicts, \n",
    "                               dfdata, is_split=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = simulatinator(theta_hat)\n",
    "\n",
    "#dft = simulatinator(torch.tensor([[-0.1006,  0.0507,  2.7590,  1.0042,  1.4923,  0.5086,  0.3, 0.06]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaa467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484790",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.4, 0.4, 20)\n",
    "\n",
    "#plt.hist(dft.loc[dft.HOST_LOGMASS < 10].c.values, histtype='step', bins=bins, label=\"sim output, low mass\", density=True)\n",
    "#plt.hist(dft.loc[dft.HOST_LOGMASS > 10].c.values, histtype='step', bins=bins, label=\"sim output, high mass\", density=True)\n",
    "plt.hist(dft.c.values, histtype='step', bins=bins, label=\"sim output, all mass\", density=True)\n",
    "\n",
    "plt.hist(dfdata.c.values, histtype='step', bins=bins, label=\"data\", density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-4, 10, 20)\n",
    "\n",
    "#plt.hist(dft.loc[dft.HOST_LOGMASS < 10].c.values, histtype='step', bins=bins, label=\"sim output, low mass\", density=True)\n",
    "#plt.hist(dft.loc[dft.HOST_LOGMASS > 10].c.values, histtype='step', bins=bins, label=\"sim output, high mass\", density=True)\n",
    "plt.hist(dft.MURES.values, histtype='step', bins=bins, label=\"sim output, all mass\", density=True)\n",
    "\n",
    "plt.hist(dfdata.MURES.values, histtype='step', bins=bins, label=\"data\", density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"MURES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(18, 26, 20)\n",
    "\n",
    "plt.hist(dft.mB.values, histtype='step', bins=bins, label=\"sim output\", density=True)\n",
    "plt.hist(dfdata.mB.values, histtype='step', bins=bins, label=\"data\", density=True)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"mB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159335c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 0.6, 20)\n",
    "\n",
    "plt.hist(dft.loc[dft.HOST_LOGMASS < 10].SIM_EBV.values, histtype='step', bins=bins, label=\"low mass output\", density=True)\n",
    "plt.hist(dft.loc[dft.HOST_LOGMASS > 10].SIM_EBV.values, histtype='step', bins=bins, label=\"high mass output\", density=True)\n",
    "\n",
    "\n",
    "plt.hist(dfdata.SIM_EBV.values, histtype='step', bins=bins, label=\"data\", density=True)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"E(B-V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe921b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a35099c0",
   "metadata": {},
   "source": [
    "# Calibrate some posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e411c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.diagnostics import run_sbc\n",
    "from sbi.analysis.plot import sbc_rank_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a347fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain your `posterior_estimator` with NPE, NLE, NRE.\n",
    "posterior = inference.build_posterior()\n",
    "\n",
    "num_sbc_samples = 200  # choose a number of sbc runs, should be ~100s\n",
    "prior_samples = prior.sample((num_sbc_samples,))\n",
    "prior_predictives = batched_simulator(prior_samples)\n",
    "\n",
    "num_posterior_samples = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a3203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08417a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79117cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks, dap_samples = run_sbc(\n",
    "    prior_samples,\n",
    "    prior_predictives,\n",
    "    posterior,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    use_batched_sampling=True, # `True` can give speed-ups, but can cause memory issues.\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sbi\n",
    "\n",
    "print(\"joblib version:\", joblib.__version__)\n",
    "print(\"sbi version:\", sbi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3860f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = sbc_rank_plot(\n",
    "    ranks,\n",
    "    num_posterior_samples,\n",
    "    plot_type=\"cdf\",\n",
    "    num_bins=20,\n",
    "    figsize=(5, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce150d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=None,  # by passing None we use a heuristic for the number of bins.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219abaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f55b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flat histogram → well-calibrated.\n",
    "\n",
    "#U-shaped → posteriors too narrow.\n",
    "\n",
    "#Bell-shaped → posteriors too wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tarp_samples = 200  # choose a number of sbc runs, should be ~100s\n",
    "# generate ground truth parameters and corresponding simulated observations for SBC.\n",
    "thetas = prior.sample((num_tarp_samples,))\n",
    "xs = batched_simulator(thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14497208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.diagnostics import check_sbc, check_tarp, run_sbc, run_tarp\n",
    "# the tarp method returns the ECP values for a given set of alpha coverage levels.\n",
    "ecp, alpha = run_tarp(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    references=None,  # will be calculated automatically.\n",
    "    num_posterior_samples=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "atc, ks_pval = check_tarp(ecp, alpha)\n",
    "print(atc, \"Should be close to 0\")\n",
    "print(ks_pval, \"Should be larger than 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c0027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00154335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f7c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f40c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb813e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis.plot import plot_tarp\n",
    "\n",
    "plot_tarp(ecp, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tarp(\n",
    "    ecp, alpha, title,):\n",
    "    \"\"\"\n",
    "    Plots the expected coverage probability (ECP) against the credibility\n",
    "    level,alpha, for a given alpha grid.\n",
    "\n",
    "    Args:\n",
    "        ecp : numpy.ndarray\n",
    "            Array of expected coverage probabilities.\n",
    "        alpha : numpy.ndarray\n",
    "            Array of credibility levels.\n",
    "        title : str, optional\n",
    "            Title for the plot. The default is \"\".\n",
    "\n",
    "     Returns\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The figure object.\n",
    "        ax : matplotlib.axes.Axes\n",
    "            The axes object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax: Axes = plt.gca()\n",
    "\n",
    "    ax.plot(alpha, ecp, color=\"blue\", label=\"TARP\")\n",
    "    ax.plot(alpha, alpha, color=\"black\", linestyle=\"--\", label=\"ideal\")\n",
    "    ax.set_xlabel(r\"Credibility Level $\\alpha$\")\n",
    "    ax.set_ylabel(r\"Expected Coverage Probability\")\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_title(title or \"\")\n",
    "    ax.legend()\n",
    "    return fig, ax  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13225d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590371c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc73dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = inference.summary[\"training_loss\"]\n",
    "val_losses = inference.summary[\"validation_loss\"]\n",
    "\n",
    "# Plot\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.ylim([-10,10])\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd775038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ac514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852dc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870fb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro.infer.autoguide.initialization import init_to_median\n",
    "\n",
    "def distancinator(x0_obs, x0_err, x1_obs, x1_err, c_obs, c_err, dist_mod):\n",
    "\n",
    "    n = dist_mod.shape[0]\n",
    "\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0.1, 1.0))\n",
    "    beta = pyro.sample(\"beta\", dist.Normal(2.0, 3.0))\n",
    "    M = pyro.sample(\"M\", dist.Uniform(-21.5, -17.0))\n",
    "    sigma_int = pyro.sample(\"sigma_int\", dist.HalfNormal(0.3))\n",
    "\n",
    "    with pyro.plate(\"sne\", n):\n",
    "        #log10_x0 = pyro.sample(\"log10_x0\", dist.Uniform(-6.0, 0.0))\n",
    "        #x0_true = 10.0 ** log10_x0\n",
    "        log10_x0 = pyro.sample(\"log10_x0\", dist.Normal(-3.0, 2.0))\n",
    "        x0_true = torch.pow(10.0, log10_x0)\n",
    "\n",
    "        pyro.sample(\"x0_obs\",\n",
    "                    dist.Normal(x0_true, x0_err),\n",
    "                    obs=x0_obs)\n",
    "\n",
    "        correction = alpha * x1_obs - beta * c_obs - M\n",
    "\n",
    "        mag_err = (2.5 / torch.log(torch.tensor(10.0))) * (x0_err / x0_true)\n",
    "        total_err = torch.sqrt(mag_err**2 + sigma_int**2 + x1_err**2 + c_err**2)\n",
    "\n",
    "        mean_mag = -2.5 * torch.log10(x0_true) + 10.635 + correction\n",
    "        \n",
    "        pyro.sample(\"cosmo\",\n",
    "                    dist.Normal(mean_mag, total_err),\n",
    "                    obs=dist_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_distance(NUM_WARMUP = 50, NUM_SAMPLES = 150, NUM_CHAINS = 1):\n",
    "    nuts_kernel = NUTS(\n",
    "        distancinator,\n",
    "        jit_compile=True,\n",
    "        init_strategy=init_to_median(),\n",
    "        max_tree_depth=10\n",
    "    )\n",
    "    \n",
    "    salt_mcmc = MCMC(\n",
    "        nuts_kernel,\n",
    "        warmup_steps=NUM_WARMUP,\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        num_chains=NUM_CHAINS\n",
    "    )\n",
    "    \n",
    "    return salt_mcmc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_mcmc = start_distance()\n",
    "\n",
    "salt_mcmc.run(\n",
    "    torch.tensor(dfdata.x0.values),\n",
    "    torch.tensor(dfdata.x0ERR.values),\n",
    "    torch.tensor(dfdata.x1.values),\n",
    "    torch.tensor(dfdata.x1ERR.values),\n",
    "    torch.tensor(dfdata.c.values),\n",
    "    torch.tensor(dfdata.cERR.values),\n",
    "    torch.tensor(dfdata.MU.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blep = salt_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance(mcmc, df_tensor):\n",
    "    \n",
    "    x1_obs = df_tensor['x1'] ; c_obs = df_tensor['c'] ; mB_obs = df_tensor['mB']\n",
    "    \n",
    "    nuisance = mcmc.get_samples()\n",
    "    beta = nuisance['beta'].mean() ; alpha = nuisance['alpha'].mean() ; M0 = nuisance['M'].mean()\n",
    "    \n",
    "    correction = alpha * x1_obs - beta * c_obs - M0 + mB\n",
    "        \n",
    "    MURES = df_tensor['MU'] - correction\n",
    "    \n",
    "    return  MURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd40bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripp = dfdata.mB + float(blep['alpha'].mean())*dfdata.x1.values - float(blep['beta'].mean())*dfdata.c.values + float(blep['M'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0031554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(tripp)\n",
    "#plt.hist(tripp_t)\n",
    "dfdata['MURES'] = tripp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b0ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_mcmc = start_distance()\n",
    "\n",
    "salt_mcmc.run(\n",
    "    torch.tensor(dft.x0.values),\n",
    "    torch.tensor(dft.x0ERR.values),\n",
    "    torch.tensor(dft.x1.values),\n",
    "    torch.tensor(dft.x1ERR.values),\n",
    "    torch.tensor(dft.c.values),\n",
    "    torch.tensor(dft.cERR.values),\n",
    "    torch.tensor(dft.MU.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blep = salt_mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripp_t = dft.mB + float(blep['alpha'].mean())*dft.x1.values - float(blep['beta'].mean())*dft.c.values + float(blep['M'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00768533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['MURES'] = tripp_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b0364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
